from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, udf
from pyspark.sql.types import ArrayType, StructType, StructField, StringType
# from .parse import parse_markdown_structure
# from .chunk import split_into_semantic_chunks


class DeepIngestor:
    def __init__(self, app_name="DeepLearner-V3"):
        self.spark = SparkSession.builder.appName(app_name).getOrCreate()

    def run_pipeline(self, input_path):
        """
        æ ¸å¿ƒæµæ°´çº¿ï¼šæ§åˆ¶æ•°æ®çš„è¯»å–ã€è½¬æ¢å’Œå±•ç¤º
        """
        # è¯»å–
        raw_rdd = self.spark.sparkContext.wholeTextFiles(input_path)
        
        # åˆ†å¸ƒå¼è½¬æ¢
        chunks_rdd = raw_rdd.flatMap(lambda x : DeepIngestor.process_file_content(x))
        
        # è½¬ä¸ºç»“æ„åŒ– DataFrame
        df = chunks_rdd.toDF() # è‡ªåŠ¨æ¨å¯¼ schema
        
        # å±•å¼€åµŒå¥—çš„ metadata åˆ—ï¼Œæ–¹ä¾¿è§‚å¯Ÿ
        return df.select(
            col("id").alias("doc_id"),
            col("text").alias("content"),
            col("metadata.h1").alias("h1"),
            col("metadata.h2").alias("h2"),
            col("metadata.source").alias("source")
        )

    @staticmethod
    def process_file_content(path_content):
        """
        ç»„åˆæ¨¡å— B å’Œ Cï¼šè¿™ä¸ªå‡½æ•°å°†è¢«åˆ†å‘åˆ°å„ä¸ªè®¡ç®—èŠ‚ç‚¹æ‰§è¡Œ
        """
        path, content = path_content
        from .parse import parse_markdown_structure
        from .chunk import split_into_semantic_chunks
        # 1. å…ˆè§£æç»“æ„
        structured_data = parse_markdown_structure(content)
        # 2. å†æ‰§è¡Œåˆ†å—
        chunks = split_into_semantic_chunks(structured_data)
        
        # 3. æ³¨å…¥æ–‡ä»¶åä¿¡æ¯
        for c in chunks:
            c['metadata']['source'] = path
        return chunks
# --- æ‰§è¡Œå…¥å£ ---
if __name__ == "__main__":
    pipeline = DeepIngestor()
    # 1. è¿è¡Œæ¸…æ´—æµæ°´çº¿
    final_df = pipeline.run_pipeline("data/knowledge.txt")
    
    if final_df:
        # 2. å°† Spark DataFrame è½¬ä¸º Python åˆ—è¡¨ (ä¸­å°è§„æ¨¡æ•°æ®ç›´æ¥ collect)
        # å¦‚æœæ•°æ®é‡æå¤§ï¼Œå»ºè®®åœ¨ run_pipeline ä¸­ä½¿ç”¨ foreachPartition åˆ†ç‰‡å†™å…¥
        data_to_write = [row.asDict() for row in final_df.collect()]
        
        # 3. è°ƒç”¨åŒè·¯å†™å…¥æœåŠ¡
        from .dual_writer import DualWriter
        writer = DualWriter()
        writer.write_all(data_to_write)
        
        print("\nğŸš€ [Success] å…¨ä½“ Ingestion ä»»åŠ¡å®Œæˆï¼")